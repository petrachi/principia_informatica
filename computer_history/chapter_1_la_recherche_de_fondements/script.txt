Alors aujourd'hui, on va parler de l’histoire de l’informatique, des années pionnières de l’informatique, et donc on va parler de tous ces personnages, de tous ces inventeurs qui ont participé à l’élaboration des premières machines. On parlera bien entendu d’Alan Turing, de John Von Neumann, de Mauchly, Eckert, Williams, Kilburn, il y aura Max Newman, et quelques autres encore que je n’ai pas le temps de citer ici. 

Mais juste avant de commencer, en guise d’introduction, j'aimerais vous répéter ce que m'a dit Jean Lassègue, qui a publié un livre sur Alan Turing aux éditions Les belles lettres, et qui me disait : l'histoire de l'ordinateur peut-être soit vue d'un point de vue théorique. C’est à dire que les premiers ordinateurs seraient le fruit d'une réflexion très théorique menée principalement par Turing et Von Neumann. Soit, l'autre version de la même histoire, c'est de considérer qu'il s'agit essentiellement d'une affaire d'ingénierie, c’est à dire la mise ou point et la perfection de machines calculantes.

Alors vous avez bien compris, on a d’un côté on a la théorie, et de l’autre la pratique. Alors on va explorer ces deux points de vue, et pour commencer, je vous propose de remonter dans les années 1880 pour se placer d’abord du point de vue théorique de cette histoire, que nous allons voir à travers le travail de David Hilbert et de sa participation à ce que j’appellerais la grande quête scientifique du début du 20ème siècle, c’est la quête du fondement mathématique.

Donc David Hilbert, on l’a bien compris, c’est un mathématicien, mais c’est un mathématicien allemand, il a fait ses études à l’université de Köningsberg, qui est l’une des universités les plus réputés du pays, et il a fait là bas deux rencontres qui vont être importantes pour sa carrière de mathématicien, puisque c’est en discutant avec Adolf Hurwitz, qui est un de ses professeurs à l’université, et avec Hermann Minkowski, un autre étudiant, que la carrière de Hilbert va prendre la direction que l’on connaît aujourd’hui, et que je m’apprête à vous raconter.

Alors, on a un témoignage direct de Hilbert qui nous raconte cette époque à Köningsberg : "Ainsi, alors que j'étais encore étudiant, j'ai été poussé par Hurwitz au coeur des débats scientifiques et j'ai eu la chance, en étant avec lui, d'apprendre à connaître, d'une façon plaisante, les deux écoles opposées mais parfaitement complémentaires, l'école géométrique de Klein et l'école algébrique-analytique de Berlin. Lorsqu'il revenait dans sa famille à Köningsberg, le génial Minkowski, avec qui je m'étais déjà lié, se joignait à nous. Au cours de ces innombrables promenades, jour après jour, pendant huit années, nous avons fouillé tous les coins du savoir mathématique."

Il passe donc huit ans à Köningsberg, d’abord, quatre ans d’études, il est diplômé en 1885, et puis, sous l’impulsion d’Hurwitz, il va voyager afin de rencontrer certains grands mathématiciens de son époque, il ira voir Felix Klein, Henri Poincaré à Paris, Kronecker à Berlin. Après un an de voyages, il retourne à l’université de Köningsberg afin d’obtenir son habilitation d’enseignant, il devient donc « Privatdozen », ce qui en français peut se traduire par « Assistant », et qui est un poste de préambule à celui de professeur.

Le « Privatdozen » peut enseigner mais ne touche pas de rémunération fixe, à la place il est directement payé par les élèves qui choisissent d'assister à ses cours, puisque, il faut le rappeler, il ‘y a pas à l’université de classes imposés. Les étudiants sont laissés totalement libres de faire ce qu’ils veulent, d’assister aux classes qu’ils choisissent pendant les quatre ans de cursus, jusqu’au seul et unique l’examen final.

Alors ce statut de « Privatdozen », qui n’est pas très confortable d’un point de vue financier, laisse cependant suffisamment de temps libre, assez en tout cas pour permettre à Hilbert de démarrer un travail de recherche, et en particulier, il va s’intéresser à la théorie des invariants qui depuis vingt ans est au point mort.

Le problème principal de la théorie des invariants, c'est d’identifier certaines propriétés, qui sont invariantes, pour des formes qui évoluent dans des espaces à plusieurs dimensions. 

Alors, bien entendu, quand on parle de théorie des invariants, on ne peux pas ne pas citer Gordan, puisque c’est Gordan, surnommé le « roi de la théorie des invariants », qui en 1869 à démontré que l'anneau des invariants des polynômes homogènes à deux variables de degré fixé est de type fini.

En clair, il à résolu le problème des invariants pour un espace en deux dimensions. Mais ce résultat de Gordan est aussi le principal obstacle de la théorie des invariants, puisque la méthode de Gordan nécessite des calculs qui sont d’une grande complexité, et qui est impossible à étendre à un nombre plus important de variables. Comprenez, pour les espaces à trois dimensions, quatre dimensions et plus, la méthode de Gordan est inutilisable.

Alors Hilbert, qui s'attaque à ce problème, va modifier son point de vue et va appliquer ce qu’on appelle en mathématique la méthode abstraite. Plutôt que de calculer, Hilbert va étudier les relations entre les différents objets mathématiques, et il va démontrer, à partir de cette étude, qu’il existe une famille génératrice de forme.

Le problème des invariants est donc presque résolu puisque les propriétés invariantes sont les propriétés de la famille génératrice dont Hilbert vient de montrer l’existence, et donc, si on peut calculer la famille génératrice, on aura du même coup les propriétés invariantes. Hélas, Hilbert ne sait pas encore comment calculer cette famille génératrice de formes.

Alors il va tout de même publier ses premiers résultats dans un article intitulé "De la théorie des formes algébriques", on est là en 1890, et Gordan, qui est sans doute un peu perplexe devant cette absence totale de calcul commentera : "Ce n'est pas des mathématiques, c'est de la théologie". 

Pourtant, Hilbert, qui ne va pas s’arrêter là, continue à travailler sur ce problème et trois ans plus tard, il publie à nouveau, et cette fois, il fournit une méthode pour calculer cette fameuse famille génératrice de forme. Gordan sera alors abrogé d’admettre : "La théologie à parfois ses mérites".

De son côté, Hilbert écrit à Minkowski, il lui dit : "Je quitte maintenant le domaine des invariants définitivement et je ne m'occuperais plus que de théorie des nombres". C'est aussi parce que l'association des mathématiciens Allemands vient de lui demander un rapport sur la théorie des nombres algébriques, puisque Kummer et Kronecker, qui sont les deux grandes figures du domaine, n’ont que peu publiés leurs travaux. 

L’ouvrage sera finalement édité en 1897, et, pour la petite histoire, deviendra plus tard l’ouvrage de référence dans les années 1920 pour les travaux d'Emmy Noether, la première femme à enseigner dans une université allemande.

Mais un autre événement à la même époque, bien plus important dans la carrière de Hilbert, c’est sa nomination en 1895 au poste prestigieux de professeur ordinaire à l’université de Göttingen, sur la recommandation de Felix Klein. C’est un grand honneur pour Hilbert, et un tournant majeur dans la vie mathématique de l’époque puisque, forte de l’influence de Klein, de Hilbert, et un peu plus tard de Minkowski, qui arrivera à Göttingen en 1902, à la recommandation de Hilbert, que l’université va se transformer pour devenir un centre incontournable, on dit c’était comme si la ville toute entière s’était dédié au savoir mathématique, au point d’en devenir la capitale mondiale.

Alors on se doute bien qu’une université qui jouit d’une telle réputation attire de nombreux étudiants; parmi eux, Hermann Weyl, qui va écrire en 1944 : "J'entends toujours la flûte douce de Hilbert nous séduisant pour nous conduire dans la rivière profonde des mathématiques [...] Les portes d'un monde nouveau s'ouvraient devant moi et je n'étais pas là depuis longtemps que déjà la résolution s'était formé dans mon jeune coeur de lire tout ce que cet homme avait écrit."

À Göttingen, Hilbert va préparer un cours de géométrie, ou plutôt, un cours sur sa nouvelle axiomatisation de la géométrie, puisque c’est, après les nombres algébriques, son nouveau domaine de recherche, peut-être d’ailleurs inspiré par l’influence de Klein. Je dis peut-être parce que je n’ai n’ai aucun document pour le prouver mais on sait très bien que Klein était très attaché à la géométrie.

Alors un mot peut-être sur l’axiomatique d’abord. Il faut remonter dans l’antiquité, en 300 avant notre ère, à l’époque d’Euclide qui, dans un livre intitulé « Les Éléments », va établir un ensemble de règles pour décrire ce qu’il est juste de faire et ce qui est interdit quand on fait de la géométrie. Ces règles, c’est ce qu’on appelle les « axiomes », et l’ensemble de ces règles, c’est « l’axiomatique ».

Depuis Euclide, l’axiomatique s’est étendue à touts les domaines des mathématiques, chaque branche possède sa propre axiomatique, qui définit les différentes notions mathématiques, et comment nous pouvons les utiliser.

Il faut préciser que les axiomes, les règles de base, sont les propositions mathématiques les plus importantes, puisque les axiomes représentent tout ce qui est « vrai », et il n’y a que la vérité inscrite dans les axiomes qu’il nous est possible d’utiliser au cours d’une démonstration ou d’un raisonnement mathématique. Tous les théorèmes reposent sur les axiomes et uniquement sur les axiomes. 

Alors il faut faire extrêmement attention, pour celui qui propose une axiomatique, de choisir des axiomes qui soient suffisamment évident, et qu’ils ne soient pas contradictoires, puisqu’il faudra ensuite pouvoir convaincre le reste de la communauté des mathématiciens.

Et justement, l’axiomatisation de la géométrie d’Euclide ne convainc plus, on lui reproche un manque d’axiomes. On se retrouve parfois dans un cas où il manque des axiomes pour compléter la démonstration.

Je vous propose un exemple pour mieux comprendre le problème : Imaginez d’abord une maison, pas très grande, et à côté de la maison, une tour de plusieurs étages. Il est évident à l’observateur que la tour est plus grande que la maison. Pour le démontrer mathématiquement, il faudrait mesurer la taille des deux bâtiments. La maison fait 18m, la tour en fait 150, il suffit ensuite de comparer ces deux nombres pour se rendre compte que 150 est plus grand que 18. On a la preuve mathématique que la tour est plus grande que la maison.

Mais comprenez bien que pour comparer 150 et 18, nous avons utilisé l’algèbre, et il y a une axiomatique à l’algèbre, et dans cette axiomatique, il y a un axiome en particulier qui nous permet de comparer deux nombres entre eux. Si cet axiome n’existait pas, la preuve mathématique que nous venons de faire aurait été impossible, même s’il serait toujours aussi évident que l’un des deux bâtiments est plus grand que l’autre.

Et c’est bien ça qu’on reproche à l’axiomatisation d’Euclide, il n’y a pas suffisamment d’axiomes pour prouver mathématiquement tout ce qu’on voudrait pouvoir prouver, et c’est ce problème que Hilbert va chercher à résoudre, en proposant une nouvelle axiomatisation de la géométrie, avec l’aide de la méthode abstraite, comme il avait fait pour la problème des invariants.

L’idée de la méthode abstraite, c’est de distancer les notions mathématiques par rapport au sens usuel, au sens courant. Si je sais par exemple que « Socrates est un homme » et que « Tous les hommes sont mortels », je peux en tirer la conclusion logique que « Socrates est mortel ». Mais le nom « Socrates », qui est utilisé dans les deux propositions de départ, qui font figures d’axiomes, ce nom fait aussi référence à un personnage historique. Alors je pourrais dire que, puisque Socrates est un homme, alors Socrates est un philosophe grec, mais cette conclusion n’est plus le fruit d’une déduction logique tiré uniquement des axiomes, mais plutôt de mon expérience des livres d’histoires. 

La méthode abstraite permet d’éviter ce genre de dérives dangereuses dans le raisonnement mathématique, en ne donnant aucun sens aux notions mathématiques. Plutôt que « Socrates est un homme » et « Tous les hommes sont mortels », on aurait : « X est un A », « Tous les A sont des P », desquels on pourrait déduire que « X est un P ».

Et c’est cette démarche que Hilbert va appliquer à la géométrie, et qui est d’ailleurs est particulièrement bien illustrée par cette petite anecdote. On a Hilbert qui est assis à la terrasse d'un café avec ses étudiants, et qui il leur dit : "L'on devrait pouvoir parler en géométrie de tables, de chaises et de chopes de bière, au lieu de points, de droites et de plan".

Alors vous voyez que, dans l’esprit d’Hilbert, les notions classiques de droites, de points et de plans sont assez superflues.

Le cours de géométrie qu'il donne d'ailleurs ne les mentionne que dans son introduction : "Nous pensons trois systèmes différents de choses. Nous nommons les choses du premier système des points [...]. Nous nommons droites les choses du deuxièmes système [...]. Nous appelons plans les choses du troisième système. [...]. Entre les points, les droites et les plans, nous imaginons certaines relations que nous exprimons par des expressions telles que "être sur", "entre", "congruent". La description exacte et appropriée au but des mathématiques de ces relations est donnée par les axiomes de la géométrie".

Après l’axiomatisation de la géométrie, Hilbert s’attaque de 1901 jusqu’en 1908 à l’axiomatisation de l’analyse, et c’est au cours de ces travaux qu’il va développer ce que lui appelle la "théorie spectrale", mais qui est aujourd’hui connue plus simplement comme la "théorie des espace de Hilbert".

Ces travaux, qui offrent de nouvelle méthodes pour résoudre des équations intégrales, vont êtres au centre des travaux de physiciens dont les noms vous serons peut-être connus, Schrödinger, Bohr, Heisenberg; Heisenberg d’ailleurs qui dira : "Indirectement, Hilbert exerça la plus grande influence sur le développement de la physique [...] Les méthodes mathématiques de la mécanique quantique sont une application directe de la théorie hilbertienne des équations intégrales".

——

Je vous avait dit la dernière fois que lorsqu’on travaille à une axiomatisation, le plus difficile finalement c’est de convaincre. Et la méthode la plus efficace pour convaincre, c’est de fournir une preuve de consistance des axiomes, c’est à dire la preuve formelle que les axiomes ne sont pas contradictoires. 

Ce qu’on fait en général, et ça sera le cas pour l’axiomatique de la géométrie, et celle de l’analyse de Hilbert, c’est qu’on construit cette preuve par analogie. Il faut réussir à exprimer les axiomes dans une forme algébrique.

L’idée est que la théorie des nombres algébriques est le domaine le plus fiable des mathématiques. Alors si les axiomes de la géométrie de Hilbert par exemple, étaient contradictoires, une fois exprimés sous forme algébrique, cette contradiction s’étendrait à l’algèbre. Mais l’algèbre est sûre et peu contestable, alors cela suffit à convaincre le reste de la communauté mathématique.

Mais des paradoxes qui vont émerger en théorie des ensembles vont remettre en cause cette méthode. La théorie des ensembles est une théorie assez récente pour l’époque dont je vous parle puisque elle à été développé à partir de 1873 et jusqu’en 1897 par Georg Cantor, qui va d’ailleurs parvenir, à l’aide des ensembles, à des conclusions qui sont très surprenantes, même si le raisonnement de Cantor reste parfaitement logique. Par exemple il va montrer qu’il existe des infinis plus grands que d’autres.

Alors afin de comparer la « taille » de différents ensembles, on dispose de la notion de « cardinalité », un ensemble de trois pommes par exemple aura une cardinalité de 3. Et un ensemble de trois ballons aura également une cardinalité de 3. 

Alors on peut associer la pomme verte au ballon de foot, la pomme rouge au ballon de basket et la pomme jaune au ballon de rugby. Ainsi, chaque élément des deux ensembles est associé à un et un seul élément de l’autre ensemble, c’est ce qu’on appelle en mathématique une correspondance bi-univoque, et on peut en conclure que les deux ensembles possèdent la même cardinalité, donc, qu’ils sont de même taille.

Et c’est en comparant différents ensembles, mais des ensembles infinis cette fois, que Cantor s’est aperçu qu’il y en avait certains qui étaient plus grands que d’autres. Alors attention, tout cela peut paraître assez évident, après tout, si on prends un exemple simple, n’y a t’il pas plus des nombres entiers que de nombres entiers pairs ? D’instinct, on pourrait penser qu’il y a environ deux fois plus de nombre entiers qu’il n’y en as de nombres pairs, mais ce serait une erreur puisqu’on ne peux pas réfléchir avec l’infini comme on réfléchit avec des un ensemble fini.

D’ailleurs, je vous propose une petite expérience pour essayer de vous convaincre. Vous me donnez un nombre entiers, et je vous donne un nombre pairs. Si vous me donnez « 1 », je vous donne « 2 », si vous donnez « 2 » je donne « 4 », vous donnez « 3 » je donne « 6 », « 4 » je donne « 8 », « 5 » je donne « 10 », « 6 » je donne « 12 ». Et on pourrait continuer comme ça à l’infini. Vous ne serez jamais à court de nombres entiers, mais, de la même façon je ne serais jamais à court non plus de nombres pairs.

Ce qu’on vient de faire, en réalité, c’est une correspondance bi-univoque entre l’ensemble des nombres entiers et celui des nombres pairs, puisqu’on peut associer à chaque nombre entier « x », le nombre pair « x*2 ». Ces deux ensembles ont donc la même cardinalité, puisqu’on peut faire une correspondance bi-univoque, et on peux dire, la même taille. Il y a autant de nombres entiers que de nombres pairs, une infinité.

Par contre, Cantor va montrer que tous les ensembles infinis ne se valent pas, en particulier il va montrer qu’il y a plus de nombres réels, c’est à dire les nombres à virgules, que de nombres entiers. Et il va le montrer à l’aide d’une méthode qui sera réutilisée ensuite par de nombreux mathématiciens, c’est la méthode de « diagonalisation ». 

Pour commencer, Cantor imagine que les deux ensembles sont de même taille, donc on peut faire une association un à un de tous leurs éléments. Le nombre « 1 » peut être par exemple associé à « 0,0145078304… », en dessous de lui, le nombre « 2 » serait associé à « 0,8350927655… », le « 3 » à « 0,0000345081… », et ainsi de suite pour tous les nombres entiers et réels.

Mais Cantor va montrer que dans cette liste, qui devrait logiquement contenir tous les nombres entiers et tous les nombres réels, il manque au moins un nombre réel. Si on prends le chiffre à la première position du premier nombre réel, puis celui à la seconde position du second nombre réel, celui à la troisième position du troisième réel, et ainsi de suite pour tous les nombres réels de la liste, et qu’on modifie chacun de ces nombres en suivant une règle simple : « si c’est un 0, il devient un 1, et si ça n’est pas un 0, il devient un 0 », alors on obtient un nouveau nombre réel qui n’était pas déjà inclus dans la liste, c’est la diagonalisation.

Pourquoi je vous dis qu’il n’était pas déjà inclus dans la liste ? Tout simplement parce que ça ne peut pas être le premier nombre, puisque le chiffre à la première position est différent. Ça n’est pas non plus le second nombre puisque le chiffre à la seconde position est différent, c’’est pas le troisième, ni le quatrième, ni le cinquième, ni le sixième, ni, finalement, aucun des nombres de la liste.

Mais si il existe, puisqu’on vient de le trouver, un nombre réel qui n’était pas déjà dans la liste, ça veut donc dire, d’abord que les deux ensembles n’ont pas la même cardinalité, mais aussi qu’il y a plus de nombres réels que de nombres entiers.

L’infini des nombres réels est donc « plus grand », puisqu’il possède plus d’éléments que l’infini des nombres entiers. Et ça, je vous avoue que c’est une idée que je ne sais pas vraiment comment qualifier. L’infini, c’est déjà une notion assez particulière à s’imaginer, puisque finalement, autour de nous, dans notre vécu, dans notre expérience, rien n’est infini. Mais alors, dire qu’il existe un infini plus grand que l’infini … 

Alors c’est justement pour étudier plus encore les différentes « tailles » d’infinis que Cantor va ajouter à la théorie des ensembles ce qu’il appelle les nombres ordinaux, qui servent à décrire à la fois la taille d’une ensemble, mais aussi l'ordre de ses éléments. Ainsi, à chaque ensemble ordonné, à chaque suite d’éléments, corresponds un nombre ordinal.  Et c’est à ce moment là qu’un mathématicien italien, Cesare Burali-Forti, va déceler un problème en 1897, avec les nombres ordinaux. 

Burali-Forti cherche à calculer le nombre ordinal associé à la suite de tous les nombres ordinaux. Problème, dés qu’il calcule ce nombre ordinal, la suite de tous les nombres ordinaux, qui sert de base au calcul, n’est plus la suite de tous les nombres ordinaux, puisqu’il en manque un, celui qu’on vient de calculer. Donc le nouveau nombre ordinal ne représente finalement pas tous les nombres ordinaux mais seulement tous ceux qui le précédent.

Et c’est là le paradoxe : N’importe quelle suite d’éléments devrait être associé à un nombre ordinal, et pourtant, il y ne peut pas exister de nombre ordinal pour la suite des nombres ordinaux. Alors c’est un premier problème.

Ensuite, en 1902, quelques années plus tard donc, c’est Bertrand Russell qui va mettre à jour un second paradoxe, il va dire : l'ensemble des cuillères à thé n'est pas lui-même une cuillère à thé, il ne s'appartient donc pas à lui-même, il ne correspond pas à sa propre définition. Par contre, on peut réfléchir à l'ensemble de tous les ensembles qui ne s'appartiennent pas à eux-mêmes. Est-ce que cet exemple là s’appartient à lui-même ou pas ?

Au fond, il n’y a que deux possibilités, soit, il s'appartient à lui-même, dans ce cas il devrait correspondrait à sa propre définition, c’est à dire qu’il ne s’appartient pas à lui-même. Donc s’il s’appartient à lui-même, il ne s’appartient pas à lui-même, mais c’est impossible, donc il doit probablement ne pas s’appartenir à lui-même. Mais s'il ne s'appartient pas à lui-même, alors il mérite de faire partie de l’ensemble des ensembles qui ne s'appartiennent pas à eux-mêmes, et alors, il s'appartiendrait à lui-même.

Dans son élan, Russell va appliquer la même logique à l'ordre des concepts. "Être vert" est un concept, on peut dire qu'une pomme "est verte", mais "être vert" n'est pas vert lui-même, il ne s'applique donc pas à lui-même. C’est ce qu’on appelle un concept hétérologique.

Mais l’hétérologie est-elle elle-même hétérologique ? Si oui, alors le concept s’appliquerait à lui-même et ne serait donc pas hétérologique, mais s’il n’est pas hétérologique, alors, comme il ne s’applique pas à lui-même, il est hétérologique. C’est le serpent qui se mords la queue.

Hilbert de son côté n'est pas surpris par les réflexions de Russell. Dans une lettre à Frege, il raconte qu’un autre mathématicien est lui aussi est arrivé à la même conclusion que Russell, et que le paradoxe est déjà connu à Göttingen depuis au moins trois ou quatre ans.

C’est finalement Antoine Richard en 1905 qui va mettre à jour le dernier paradoxe de la série. Il commence par faire la liste de tous les nombres définis en moins de mille mots, puis, en empruntant la méthode de diagonalisation de Cantor, il trouve un nombre qui n’est pas déjà inclus dans la liste. Mais c’est un problème puisque la méthode de diagonalisation s’explique en moins de mille mots, donc le nombre révélé est définissable en moins de milles mots et devrait déjà être dans la liste.

Alors, on pourrait penser que le problème, finalement, c’est la théorie des ensembles, qui n’est pas très correcte, mais souvenez vous que la consistance de la théorie des ensembles repose, comme le reste des branches des mathématiques sur la consistance présumée de l’arithmétique. Alors les paradoxes trouvés en théorie des ensemble se répercutent donc sur l’arithmétique, et finalement sur l’ensemble de toutes les mathématiques.

Alors c’est un gros problème auquel vont devoir faire face maintenant les mathématiciens, puisqu’il va falloir fixer l’arithmétique et l’algèbre, refaire une nouvelle axiomatique afin de faire disparaitre les paradoxes déjà connus, et de donner une preuve de consistance de ces axiomes, afin de se prévenir contre l’apparition de nouveaux problèmes. Seulement, avant, les preuves de consistances étaient produite par analogie avec l’arithmétique et l’algèbre, et c’est une méthode qu’on ne pourra pas utiliser pour l’arithmétique elle-même. Et personne à ce moment là n’a la moindre idée de comment faire une preuve de consistance qui ne reposerait pas sur l’arithmétique et l’algèbre, alors ça sera le début de ce qu’on a appelé la quête du fondement.

——

Je vous ai dit le dernière fois que le problème maintenant, c’était de trouver un fondement solide aux mathématiques, et ce fondement repose sur une preuve de consistance des axiomes de l’arithmétique. D’ailleurs, Hilbert s'était déjà exprimé sur la nécessité de cette preuve de consistance, c’était en 1900 au congrès international des mathématiciens à Paris.

Hilbert devait y donner une conférence, un exposé général sur les mathématiques, mais c'est Minkowski, son ami de Köningsberg qui va lui donner une autre idée. Il écrit à Hilbert le 5 janvier 1900 : "Il serait intéressant de regarder dans le futur et de faire une liste des problèmes sur lesquels les mathématiciens travaillerons dans le siècle qui vient. Avec un tel sujet, on parlera encore de ton exposé dans plusieurs décennies".

L’histoire, bien entendu, lui donnera raison, et Hilbert présentera finalement une liste de 23 problèmes qui vont occuper la quasi-totalité de la recherche mathématique pendant la première moitié du 20ème siècle, et d’ailleurs, il reste encore aujourd'hui, quelques problèmes de cette liste qui ne sont toujours pas résolus.

Mais en particulier, j’aimerais vous lire le second problème de la liste, qui fait figure de précurseur à la quête du fondement, le problème est le suivant : "Peut-on prouver la cohérence de l'arithmétique ? En d'autres termes, peut-on démontrer que les axiomes de l'arithmétique ne sont pas contradictoires ?".

Hilbert va s’intéresser personnellement à ce problème et en 1904, il tente, dans le cadre d'une conférence intitulée "Sur les fondements de la logique et de l'arithmétique", une preuve de consistance, non pas pour l’arithmétique mais pour un système minimaliste d'axiomes qui ne représente que les nombres entiers, et la notion d'égalité. Cette démonstration va être critiquée par Henri Poincaré, mais il faut dire que les deux hommes ne partagent pas les même convictions par rapport aux mathématiques.

De son côté, Hilbert estime que les objets mathématiques, les nombres par exemple, existent, qu’ils sont réels. Un nombre, n’importe lequel, existe, même si personne ne l’a jamais écrit, même si personne ne l’a jamais calculé, ce nombre, et tous les autres nombres, les fonctions, les droites, les vecteurs, tout cela existe indépendamment des mathématiciens. Pour Poincaré par contre, c’est l’inverse, pour lui les nombres sont construits par les mathématiciens, un nombre n’existe qu’une fois que le mathématicien l’a calculé, ou définit.

Mais ce qu’il faut bien comprendre c’est que ces deux points de vue ont une grande influence sur la façon de faire des mathématiques. Pour Poincaré par exemple, on ne peux pas parler d'infini. Comme les nombres n'existent pas, il est impossible d’imaginer l’ensemble de tous les nombres, ou même, l’ensemble de tous les ensemble, comme dans le paradoxe de Russell.

Pour répondre au problèmes que posent les paradoxes, Poincaré prône le constructivisme. Pour le paradoxe de Richard par exemple, le dernier des trois, le paradoxes existe simplement parce que Richard fait une mauvaise utilisation des mathématiques. Pourquoi est-ce que Richard pense que le nombre obtenu par diagonalisation aurait dû être dans la liste ? Certes il est définissable en moins de milles mots, mais au moment de construire la liste, ce nombre n’existait pas encore, il ne pouvait donc pas y figurer.

C’est ce que Poincaré appelle le principe du cercle vicieux : un objet ne devrait pas s’auto-référencer dans sa propre définition, il ne devrait pas avoir besoin de déjà exister pour pouvoir être définit. C’est le cas dans le paradoxe de Richard, et c’est le cas dans la démonstration de consistance de Hilbert, qui utilise des nombres pour définir les nombres.

Depuis les critiques de Poincaré et la mise en évidence du principe du cercle vicieux, il est maintenant clair que pour donner une preuve de consistance de l’arithmétique, il faudra différencier deux choses : d’un côté, les notions que l’on veut prouver, et de l’autre, celles qui serviront à construire cette preuve.

Alors Hilbert, qui a appris de ces critiques, va choisir de diviser l'arithmétique en deux domaines. D'un côté, la mathématique formelle. C’est une mathématique totalement abstraite, donc détachée du sens usuel, et qui décrit l'ensemble complet des mathématiques contemporaines. Et d'un autre côté, Hilbert construit la mathématique finitiste, qui se limite à utiliser les notions qui sont immédiatement évidentes et acceptées de tous : les nombres entiers, les opérations arithmétiques de bases, etc; et qui laisse de côté tout le reste, toutes les notions sur lesquelles les mathématiciens ne s’entendent pas, comme la notion de l’infini.

L’objectif d’Hilbert, avec ces deux mathématiques, est de réussir à donner une preuve de consistance pour la mathématique formelle à l’aide de la mathématique finitiste que personne ne conteste. Comme il le dit, , il s'agit de "s'assurer par le fini le droit d'opérer sur l'infini". Mais Hilbert n’est pas le seul à chercher une solution au problème du fondement.

Il y a par exemple en Angleterre Bertrand Russell et Alfred Whitehead qui vont tenter une axiomatisation de l’ensemble des mathématiques, inspirés d’un précédent article de Russell : « La logique mathématique fondée sur la théorie des types ».

Cette théorie des types cherche à hiérarchiser les différentes notions des mathématiques. Ainsi, les notions de rang 1 s’appliquent uniquement aux notions de rang 2, les notions de rang 2 s’appliquent aux notions de rang 3, etc. Et cette hiérarchie permet d'éviter qu'une notion ne s'applique à elle-même, et donc d’éviter le cercle vicieux de Poincaré.

Russell et Whitehead vont donc publier leur axiomatique globale des mathématiques en trois volumes, intitulés « Principia Mathematica ». Le travail est véritablement colossal, il leur faudra plus de dix ans pour compléter leur oeuvre, et on trouve par exemple, pour vous donner juste une idée, plus de quatre cents cinquante pages afin de prouver que 1 + 1 = 2. 

Pourtant, il va manquer aux « principas » un élément essentiel, une preuve de consistance des axiomes. Pour Russell, qui est celui qui s’occupe des questions d’ordre épistémologiques, philosophiques on pourrait dire, les axiomes présentés sont des notions intuitives, évidentes, et nécessairement vraies. Une preuve de consistance ne serait, pour lui, qu’une simple validation de l’évidence, de la vérité, des axiomes, et n’est finalement pas très importante.

Hilbert commentera à propos des « principias » quelques années plus tard : "La théorie des fondements de Russell et Whitehead [...] fait reposer les mathématiques sur l'axiome de l'infini et sur un axiome dit de réductibilité. Or ces deux axiomes sont d'authentiques hypothèses qui ne peuvent s'appuyer [...] sur une preuve de consistance [... et] dont la validité universelle reste même ouverte au doute". Alors il est clair que l’évidence des axiomes ne fait pas l’unanimité.

Un peu plus loin, aux Pays-Bas, on trouve Brouwer, qui lui, place l’évidence immédiate et l’intuition au coeur des mathématiques. Pour Brouwer et le reste de ce qu’on appelle les « intuitionnistes », il faut bannir des mathématiques tout ce qui n’est pas immédiatement évident.

Les paradoxes étaient un avertissement : un usage abusif des règles de la logique, qui font abstraction du sens des notions, crée des énoncés vides de sens, et finalement, des paradoxes.

Brouwer en vérité est assez proche des positions de Poincaré. Comme lui il a une vision constructiviste des mathématiques, il veut supprimer la notion d’infini, et même si, pour cela il doit renoncer à certains outils très pratiques en mathématiques, comme le principe du « tiers exclu ».

Ce principe, qui repose sur les notions d'infini, et donc d'existence en soi des objets mathématiques, permet de dire que soit une propriété mathématique est vraie, soit son opposée est vraie, il n’y a pas de troisième possibilité. Avec le tiers exclu, un problème mathématique est donc soit vrai, soit faux, et c'est ce qui fera dire à Brouwer que : "La question de la validité du tiers exclu équivaut à celle de la possibilité de problèmes mathématiques non résolubles".

Alors il faut se souvenir que Hilbert avait défendu sa conviction dans la résolubilité de tout problème mathématique. Il avait dit au congrès international de 1900 : « Jamais le mathématicien ne sera réduit à dire Ignorabimus ! ». Ignorabimus, la citation latine qui signifie « on ne sait pas, et on ne saura pas ». Donc jamais le mathématicien ne sera réduit à dire « on ne sait pas et on ne saura pas », autrement dit, il n’y a pas de problème mathématique impossible à résoudre.

Cependant, l’intuitionnisme va gagner en influence, au point même de convaincre Herman Weyl, l’ancien étudiant d’Hilbert qui s'oppose maintenant à son professeur, je le cite : "Si les mathématiques doivent rester une préoccupation culturelle sérieuse, alors un certain sens doit être attaché au jeu de formules de Hilbert, et je ne vois qu'une seule possibilité de lui attribuer un sens intellectuel indépendant. En physique théorique, nous avons devant nous le grand exemple d'une connaissance d'un caractère tout à fait différent de la connaissance commune qui exprime purement ce qui est donné dans l'intuition".

Il faut comprendre qu’en physique, l’intuition, c’est à dire l’expérience, prends le pas sur les formules de calculs. Pourtant les mathématiques, qui représentent la pensée logique, fonctionnent sur le modèle inverse.

Alors Hilbert répondra aussi aux intuitionnistes, et leur dira : "Oter le [tiers exclu] au mathématicien serait comme si on voulait enlever à l'astronome son télescope, au boxeur le droit de se servir de son poing. [... C'est] quasiment à renoncer à la science mathématique. Car que sont les misérables restes, qu'est-ce que la poignée de résultats incomplets et disparates que les intuitionnistes ont pu élaborer [...] à l'égard de l'étendue formidable de la mathématique contemporaine ?".

Alors on trouve une certaine force dans cette déclaration d’Hilbert, mais encore faudrait-il qu’il puisse proposer sa propre solution au problème du fondement, puisqu’il n’a encore aucun résultats de ce côté là pour l’instant, et c’est pour ça qu’il va fonder à partir de 1922, et ça continuera jusqu’en 1930, ce qu’on à appelé le programme de Hilbert.

——

L'objectif affiché du programme formaliste de Hilbert est d'établir la mathématique formelle, une mathématique abstraite dont on pourrait dire qu’elle est pure de forme mais vide de sens, à l’aide de la mathématique finitiste, qui n’utilise que des notions clairement établies, et acceptées par toute la communauté mathématique.

C’est ce qui fait la force du programme formaliste, d’ailleurs, pour Hilbert, formaliser "c'est dépeindre l'activité de notre intelligence", c'est "dresser un inventaire des règles d'après lesquelles fonctionne réellement notre pensée". Il se permettra même quelques commentaires à l’égard des arguments des autres mathématiciens qui se sont essayés au problème du fondement, en disant qu’il n’a "besoin ni du Bon Dieu, comme Kronecker, ni de l'hypothèse d'une faculté de conscience accordée au principe d'induction complète, comme Pointcarré, ni de l'intuition originelle de Brouwer, ni non plus des axiomes de l'infini [comme Russell et Whitehead]".

Pourtant, et malgré la force supérieure des arguments philosophiques du programme formaliste, les mathématiciens  qui sont engagés dans le programme, il y a Hilbert bien entendu, mais il y a aussi Bernays, Von Neumann, Ackermann; ces mathématiciens n’ont toujours pas obtenus de résultats satisfaisants et toutes les différentes tentatives d’axiomatisation ou de preuve de consistance ont échouées.

Alors en 1928, Hilbert va prendre la parole au congrès international de Bologne, et il va exposer devant ses pairs trois problèmes que cherche à résoudre le programme formaliste : L'axiomatique formelle est-elle complète ? consistante ? et décidable ?

Il faut expliquer ce que cela veut dire. D’abord, l’axiomatique formelle est-elle complète ? C'est à dire qu’il faut pouvoir prouver que n'importe quelle formule crée à partir des axiomes est nécessairement soit démontrable, soit réfutable. Il faut y voir la volonté d’une validation du principe du tiers exclu, et de la conviction dans la résolubilité de tout problème mathématique. On se souvient de la citation de 1900, « Jamais le mathématicien ne sera réduit à dire Ignorabimus ».

Ensuite, l’axiomatique formelle est-elle consistante ? C’est le coeur du programme formaliste et de la quête du fondement, il s’agit toujours de prouver que les axiomes ne sont pas contradictoires.

Et enfin, l’axiomatique formelle est-elle décidable ? C’est à dire, est-ce qu'il existe une méthode effective, qui permette de décider si une formule donnée est démontrable ou réfutable ? Hilbert cherche en fait une formule mathématique qui permettrait de démontrer quasi automatiquement tous les théorèmes qui existent.

Alors il est bien entendu que Hilbert espère apporter une réponse positive à ces trois questions. Et c'est même à cette condition seulement que pourrait se concrétiser le programme formaliste. D’ailleurs, en 1928, on peut-être assez optimiste. Au cours de la conférence, Hilbert à annoncé qu'Ackermann et Von Neumann sont parvenus à prouver la consistance de l'axiomatique formelle, et aussi qu’Ackermann et lui-même seront bientôt en mesure d’apporter la preuve de la complétude des « prédicats du premier ordre », et même si le prédicats ne représentent qu’une partie de l’axiomatique formelle, c’est une partie très importante puisque les prédicats du premier ordre décrivent l’ensemble des règles de logiques applicables dans une démonstration mathématique.

Alors ils sont nombreux ceux qui écoutent la conférence de Hilbert, mais en particulier il faut noter la présence à Bologne d’un jeune mathématicien Autrichien qui va prêter beaucoup d’attention à l’exposé d’Hilbert, ce mathématicien, c’est Kurt Gödel.

Alors j’ai dit qu’il était Autrichien mais on peut ouvrir là une petite parenthèse sur la nationalité de Kurt Gödel. Il est né Austro-Hongrois, puis sera naturalisé Tchécoslovaque à la fin de la première guerre mondiale, au moment de la dissolution de l'Autriche-Hongrie. Toujours attaché à ses racines, il va prendre la nationalité Autrichienne en 1929, puis devra devenir Allemand en 1938, à cause de l’annexion de L’Autriche par L’Allemagne. Quelques années plus tard, de peur d’être enrôlé dans l’armée Allemande à cause de sa santé fragile, il part aux États-Unis. Pour obtenir sa nouvelle nationalité, il doit passer devant un juge, qui n’acceptera la candidature de Gödel que grâce à la réputation d’Albert Einstein, avec qui Gödel est très ami, puisque Gödel, qui avait étudié avec attention la constitution en vue de l’entretient était en train de démontrer au juge qu’un régime dictatorial pouvait émerger aux États-Unis en toute légalité.

Alors pour revenir à notre histoire, Je vous avais dit que Hilbert et Ackerman pensaient pouvoir démontrer la complétude des prédicats du premier ordre, mais c’est en fait Gödel qui apportera cette preuve, on est en 1929. Et puis, un an plus tard, en août 1930, se tient une conférence à Köningsberg. Plusieurs points de vue sur le problème du fondement sont exposés, Heyting parle de l'intuitionnisme, Von Neumann défends le formalisme, et Hilbert annonce sa retraite avec un discours qui se termine par : « Nous devons savoir. Nous le saurons! ». Des mots qui s’opposent bien entendu à l’Ignorabimus qu’il avait déjà attaqué en 1900.

Alors, si je vous parle de cette conférence, c’est parce que sur les côtés de la conférence, à la faveur d’une discussion informelle, Gödel va présenter un théorème, qu’il a trouvé quelques jours plus tôt on imagine, et qu’il n’a pas encore publié à quelques-uns de ses collègues, parmi lesquels figure John Von Neumann. Il dit : « Si l'arithmétique élémentaire est ω-consistante, elle comporte des formules fermées qui ne sont ni démontrables, ni réfutables à partir des axiomes ». Et Von Neumann, qui, on le rappelle, est au centre du programme formaliste, aura ce mot : « Tout est fini » !

C’est d’ailleurs le seul, à ce moment là, à réaliser les conséquences du théorème de Gödel, qui, exposé simplement, dit que n’importe quelle axiomatique est nécessairement soit incomplète, soit inconsistante. Alors pour le programme formaliste qui voulait prouver à la fois la complétude et la consistance de l’axiomatique formelle, cet objectif est en fait irréalisable.

Et pourtant, l’idée est simple, Gödel considéré même qu’il s’agit d’une "conséquence presque triviale" des travaux de Skolem. En fait, Gödel montre qu’il est possible dans n’importe quelle axiomatique, à la condition que cette axiomatique soit suffisamment expressive pour représenter les nombres entiers et les opérations d’addition et de multiplication, c’est à dire l’arithmétique élémentaire, il est possible de créer une proposition qui dise : « Je ne suis pas démontrable ».

À partir de là il n’y a plus que deux possibilités, soit la formule est démontrable, mais alors, nous avons un système d’axiome qui démontre des choses qui sont fausses. Et c’est la catastrophe puisque dans ce cas là le système n’est pas consistant, et donc inutilisable. 

L’autre solution, c’est que la formule ne soit pas démontrable, mais alors, elle est vraie. Donc on se retrouve avec une formule qui est vraie mais qui n’est pas démontrable, le système est incomplet.

Deux mois plus tard, le 20 Novembre, Von Neumann écrit à Gödel, il lui dit qu'il à trouvé un second théorème à partir du premier : "la consistance d'un système ne peut pas être établie au moyen de raisonnements qui se laisseraient formaliser dans le système".

C’est une nouvelle catastrophe pour le programme formaliste, puisque l’idée de Hilbert était d’apporter la preuve de consistance de l’axiomatique formelle à l’aide de la mathématique finitiste. Mais comme la mathématique finitiste, qui ne représente qu’une partie de l’arithmétique, est formalisable dans l’axiomatique formelle, qui elle représente toute l’arithmétique, alors il est impossible d’utiliser la mathématique finitiste pour apporter la preuve de consistance de l’axiomatique formelle.

En fait Gödel était déjà arrivé à la même conclusion quelques jours plus tôt, et il avait déjà envoyé un article avec les deux théorèmes à une revue mathématique Viennoise, qui le publiera finalement en Janvier 1931.

Si on en croit Bernays, Hilbert entra dans une colère noire à l’instant où il découvre les deux théorèmes. Peut-être parce qu’il n'avait lui-même découvert ces deux théorèmes. Ou peut-être parce que ces deux théorèmes marquent l’échec de son programme.

Et pourtant, Gödel n’est pas du même avis, il dit : "il reste un espoir pour que dans le futur on puisse trouver des méthodes satisfaisantes [...] dépassant les limites du système [finitiste de Hilbert] et permettant de fonder l'arithmétique classique et l'analyse. Cette question ouvre un champ de recherches fécond". Et il ajoute : "la conviction dans la décidabilité de tout problème mathématique n'est pas ébranlée par ce résultat". 

Par ailleurs, Gödel n’est pas le seul à considérer que les théorèmes d’incomplétude ne marquent pas un point final au programme formaliste, John Von Neumann par exemple va dire: "Ce résultat imposant de l'analyse de Gödel ne doit pas être mal compris: il n'exclut pas une preuve méta-mathématique de la consistance de l'arithmétique. [...] Gödel a montré qu'aucune preuve n'est possible qui peut être représentée au sein de l'arithmétique. Son argument ne supprime pas la possibilité de preuves strictement finitiste qui ne sont pas représentés au sein de l'arithmétique. Mais personne aujourd'hui ne semble avoir une idée claire de ce qu'une preuve finitiste serait qui ne serait pas formulable dans l'arithmétique".

De toute façon, avant de clore définitivement le programme formaliste, il reste encore à résoudre la question de la décidabilité. Et cette question va prendre d’autant plus d’importance depuis les théorème de Gödel, puisqu’il y est question de trouver une « procédure effective » qui puisse décider de la démontrabilité de n’importe quelle formule. Et ce terme de « procédure effective », qui est au centre de la question de la décidabilité, est également au centre de la généralisation des théorèmes d’incomplétude de Gödel, mais il n’y a pas de définition mathématique précise pour cette idée de « procédure effective », qui en fait renvoie à l’idée de « calculabilité ». Alors il va s’agir maintenant de trouver une définition appropriée à la « calculabilité », de sorte d’identifier tout ce qui est calculable, et de laisser de côté tout ce qui ne l’est pas.

——

L’idée de « procédure effective » dans la formulation d’Hilbert fait en réalité référence à la notion de « calculabilité ». Alors depuis le 18éme siècle, les mathématiciens avaient associé l'idée de calcul avec l'idée de fonction. Mais les fonctions ont évoluées au 19ème siécle et pour ne plus décrire qu’une correspondance, une transition entre un point de départ et un point d'arrivé.

Alors on va dissocier deux types de fonctions, celles qui possèdent une procédure de calcul qui décrit la transformation effectuée par la fonction, par exemple la fonction qui, à n’importe quel nombre « x » associe le  nombre « x^2 », et les fonctions pour lesquelles on n’a pas de procédure de calcul, les fonction qui sont dites non-calculables.

L’enjeu derrière la définition du terme « procédure effective » et de la notion de « calculabilité », c’est de réussir à définir la limite entre les fonctions calculables et les fonctions non-calculables.

Le premier à trouver une définition satisfaisante de la calculabilité, c’est Alonzo Church, un américain, et pourtant, ce n’est pas sa définition qui restera dans les mémoires, mais plutôt celle d’un jeune mathématicien anglais : Alan Turing.

Mais commençons tout de même par parler des travaux d’Alonzo Church, qui est établi à l’université de Princeton dans le New Jersey, donc proche de New York, et qui va publier en avril 1936 « Un problème insoluble de la théorie des nombres élémentaires », dans lequel figure la solution au problème de la décision, basé sur un système de calcul que Church à développé quelques années auparavant, et qu’il appelle le λ-calcul.

L’idée derrière le λ-calcul, c’est que tout y décrit sous forme de fonction. Une fonction est décrite par une expression, qui contient soit un calcul, soit une autre fonction, ou bien une combinaison de ces deux éléments. 

Pour construire des fonctions de plus en plus complexes, on peut « appliquer » une fonction à une autre, c’est à dire qu’on va utiliser le résultat d’une première fonction pour le calcul d’une seconde fonction.

C'est à partir de cette base que Church définit la calculabilité, puisqu’il est possible avec le λ-calcul de construire n’importe quelle fonction qui calculable.

J’avoue préférer ne pas rentrer dans le détail du λ-calcul, si les principe peuvent paraitre simples, la lecture et l’explication de raisonnements mathématiques basés sur le λ-calcul peuvent rapidement donner une sérieuse migraine. Et c’est d’ailleurs la faiblesse du système de Church, la difficulté à lire le λ-calcul rends la vérification de son travail très difficile pour les autres mathématiciens. 

Stephen Kleene d’ailleurs, une autre mathématicien, n’est pas vraiment convaincu par la définition de Church pour la calculabilité, et ilpense même qu’une telle définition est impossible. 

Il se souvient de la méthode de diagonalisation de Cantor, et il a l’idée que, si on possédait une définition de la calculabilité, on pourrait faire la liste de toutes les fonctions calculables, puis, par diagonalisation, on pourrait créer une nouvelle fonction calculable qui ne soit pas dans la liste, et en conclure donc que la définition n’est pas bonne, puisqu’elle n’englobe pas toutes les fonctions calculables.

Il avait peut-être parlé un petit peu vite, et racontera plus tard : "Lorsque Church me proposa sa thèse [pour une définition de la calculabilité], je me mis aussitôt à la tâche de la réfuter par une diagonalisation de la classe des fonctions [calculables]. Mais, réalisant vite que la diagonalisation ne pouvait être faite de façon effective, je devins du jour au lendemain partisan de la thèse de Church".

Aujourd’hui il n’y a plus aucun doute sur la validité de la définition de Church, et pourtant, c’est la définition d’Alan Turing, qui est assez jeune en 1936 puisqu’il n’a que 24 ans, qui va marquer les esprits par sa simplicité et son évidence, mais aussi parce qu’il y a chez Turing quelques idées totalement nouvelles, et qu’on peut même qualifier de révolutionnaires, mais en tout cas qui vont marquer le 20ème siècle.

Alors en 1936, Turing à obtenu une bourse au King’s College de Cambridge, en Angleterre, qui lui permet de faire à peu près ce qu’il veut pendant trois ans, mais l’année précédente il était encore étudiant et il assistait au cours, intitulé « fondement des mathématiques », de son professeur de logique : Max Newman.

Le cours porte sur le programme formaliste de Hilbert, on y apprends aussi les théorèmes d'incomplétudes de Gödel et Newman y ajoute un commentaire personnel sur le problème de la décision, il dit : "Supposons, par exemple, que nous puissions trouver un système fini de règles qui nous permettrait de dire si une formule quelconque est ou non démontrable. Ce système contiendrait un théorème de métamathématique. Évidemment, ce théorème n'existe pas et c'est heureux, parce que s'il existait, nous aurions un ensemble mécanique de règles nous permettant de trouver la solution de tous les problèmes mathématiques et notre activité en tant que mathématiciens cesserait d'exister".

Vous avez bien entendu, « un ensemble mécaniques de règles ». C’est cette vision mécaniste du calcul, qui nous vient d’abord de John Von Neumann, "il y aurait une instruction absolument mécanique à l'aide de laquelle quiconque pourrait décider, à partir de n'importe quelle formule donnée, si elle est ou non démontrable", qui va pousser Turing à essayer de trouver la bonne machine pour définir la calculabilité.

Il y a deux citations que j’aimerais vous lire à ce sujet afin de valider ma pensée sur ce sujet, la première est de Max Newman : "et cela bien sûr conduit [Turing] au prochain défi, quel type de machine ? Et cela l'a inspiré pour essayer de définir ce que l'on voudrait dire par une machine à calculer parfaitement générale".

L'autre citation que je veux vous lire vient de William Newman, le fils de Max Newman, qui dit : "À un moment, il a posé à sa classe une question: la prouvabilité des énoncés mathématiques pourrait-elle être découvert par un procédé mécanique ? Cette question est restée dans l'esprit d'Alan, et peu à peu, il a développé le papier qui allait faire sa réputation dans le monde mathématique, et dans » une science dont je me permet de taire le nom pour l’instant, afin de conserver un peu de suspens.

C’est à la fin de l’été 1936, seulement deux mois après la publication de Church, que Turing finit la rédaction de son article : « On Computable Numbers », « Sur les nombres calculables, avec une application au problème de la décision ».

Mais il faut le signaler, Turing, quand il termine son article n’a pas encore connaissance des travaux de Church. Ce qui n’est plus le cas au moment de la publication de l’article, en janvier 1937.

Alors on va trouver des mentions du λ-calcul de Church dans l’article de Turing, dans l’introduction par exemple : "Selon ma définition, un nombre est calculable si [il] peut être écrit par une machine. [...] Dans un article récent Alonzo Church a introduit l'idée de "calculabilité effective", ce qui équivaut à ma "calculabilité", mais est définie de manière très différente. Church atteint également des conclusions similaires au sujet du problème de la décision. La preuve de l'équivalence entre "calculabilité" et "calculabilité effective" est décrite dans une annexe au présent document".

Turing va en réalité s’installer à Princeton juste après avoir terminé son article, pour travailler sous la direction de Church, à des problèmes de logiques, et il a eu le temps alors d’étudier le λ-calcul de Church, et d’ajouter une annexe à son propre article.

C’est son ancien professeur, Newman, qui à arrangé la venue de Turing à Princeton, il écrit dans une lettre à Church : « Je dois mentionner que le travail de Turing est entièrement indépendant: il a travaillé sans supervision ou critique de quiconque. Cela rend d'autant plus important qu'il devrait entrer en contact le plus tôt possible avec les principaux travailleurs de ce domaine, de sorte qu'il ne devrait pas se transformer en un solitaire confirmé ».

Alors on verra ce qu’il en deviendra plus tard, mais regardons plutôt ce qui est écrit dans ce fameux article, à commencer par la définition même de ce qu’on appelle aujourd’hui la machine de Turing. Au passage, c’est Alonzo Church qui utilise pour la première fois cette expression, puisque dans son article, Turing ne parle de ses machines qu’en terme de machines automatiques.

"On peut comparer un homme en train de calculer un nombre à une machine qui est seulement capable d'un nombre fini de conditions, qui seront appelés "m-configurations". La machine est livrée avec une "bande" (l'analogue du papier) qui la traverse, et est divisée en sections (appelé "carrés") capables chacun de porter un "symbole". A tout moment, il y a juste un carré [...] qui est "dans la machine". Nous pouvons appeler cette place le "carré lu". Le symbole sur le carré lu peut être appelé le "symbole lu". Le "symbole lu" est le seul dont la machine est, pour ainsi dire, "directement au courant". Cependant, en modifiant sa m-configuration la machine peut se rappeler efficacement certains des symboles qu'elle a "vu" précédemment. À tout moment, le comportement de la machine est déterminé par la m-configuration [...] et [...] le symbole lu. Cette paire [...] sera appelé "configuration" : ainsi la configuration détermine le comportement de la machine. Dans certaines configurations dans lesquelles le carré lu est vierge (i.e. il ne porte pas de symbole), la machine écrit un nouveau symbole sur le carré lu: dans d'autres configurations, il efface le symbole lu. La machine peut également changer le carré en cours de lecture, mais seulement en le déplaçant d'une case à droite ou à gauche. En plus de l'une de ces opérations, la m-configuration peut être modifiée. Certains des symboles écrits formeront la séquence de chiffres [...] du nombre [...] qui est calculé. Les autres ne sont que des brouillons pour "aider la mémoire". [...] Je soutiens que ces opérations comprennent toutes celles qui sont utilisés dans le calcul d'un nombre."

Turing fait une description qui est volontairement très courte, son objectif est de forcer le lecteur à se mettre à la place de la machine s’il veut en saisir le mécanisme. Alors, mettons nous à la place de la machine, et calculons.

Voici le premier exemple de calcul que nous donne Turing : "Une machine peut être construite pour calculer la séquence 01010101 ... [...] La machine possède quatre m-configurations "b", "c", "f", "e" et est capable d'écrire "0" et "1". Le comportement de la machine est décrite dans le tableau suivant dans lequel "R" signifie "la machine se déplace de sorte à lire le carré immédiatement à la droite de celui qui était auparavant lu". De même, pour "L". "E" signifie "le symbole lu est effacé" et "P" signifie "écrire". La machine démarre dans la m-configuration "b" et avec une bande vide. [b None P0, R c; c None R e; e None P1, R f; f None R b]"

Alors la machine démarre avec la m-configuration « b », et le symbole lu est vide, c’est donc la première ligne du tableau qui s’applique : La machine imprime le symbole « 0 », se décale vers la droite et passe dans la m-configuration « c ». Et on recommence, la machine est en m-configuration « c », le symbole lu est vide, c’est donc la seconde ligne du tableau qui s’applique, la machine se décale vers la droite et passe en configuration « e ».

C’est ensuite la troisième ligne du tableau qui va s’appliquer, puisqu’on a un symbole vide et la m-configuration « e », on écrit un « 1 », on se décale à droite et on passe en m-configuration « f ». Finalement, cette configuration correspond à la dernière ligne du tableau, la machine se décale à droite et passe en m-configuration « b », et la boucle va recommencer.

Alors je ne vais pas ici vous montrer d’exemples plus compliqués, mais j’aimerais insister sur une chose, la table d’instruction qui décrit le comportement de la machine est fixe, elle ne peux pas être modifié. Cette table représente les rouages, les mécanismes, la construction de la machine.

Ainsi, la machine que nous venons de simuler ne peut que calculer la séquence 01010101… De la même manière, il existe une machine capable de faire une soustraction, une pour la multiplication et une autre qui calcule les décimales de π. 

Et il devient alors possible de considérer qu'un calcul particulier est définissable par rapport à la machine de Turing qui effectue ce calcul. La séquence 010101 ... peut être définie par rapport à la machine de tout à l'heure. Et ce qui caractérise cette machine, c’est sa construction, c’est à dire sa table d’instruction.

En codant, selon un procédé particulier, chaque ligne de la table d'instruction, on peut obtienir un nombre, le « nombre de définition » , qui est la représentation numérique d’une machine de Turing, et donc, d’ un calcul en particulier.

Voici la méthode : On commence par lister les m-configurations de la machine, de sorte à pouvoir les identifier comme la "première" m-configuration, ou la "cinquième" m-configuration. On peut alors coder une m-configuration spécifique en utilisant la lettre "D", suivie autant de fois que nécessaire par la lettre « A », afin de décrire sa position dans la liste. Par exemple, la première m-configuration sera codée par "DA", la seconde sera "DAA", et la cinquième "DAAAAA".

On va procéder de la même manière pour les symboles que peut écrire la machine. On en fait la liste et puis on les code avec la lettre "D" suivi de la lettre "C". Le premier symbole sera "DC" et le troisième "DCCC".

Enfin, on va associer une lettre spécifique pour chaque opération de la machine, et on va séparer chaque ligne de la table d’instruction par un « ; », ainsi, on peut représenter la table d'instruction d'une machine par un code uniquement composé des caractères "A", "C", "D", "L", "R", "N" et ";".

Ce code est appelée la « définition standard » de la machine, et pour passer de la « définition standard » au « nombre de définition », il suffit d'associer un chiffre à chaque caractère. Le "A" est remplacé par un "1", le "C" par  un « 2 », "D" par "3", "L" par "4", "R" par "5", "N" par "6" et finalement, le « ; » est remplacé par un "7".

Si on reprends la table d'instruction de la machine qui nous as servi d’exemple, et qu’on applique cette procédure, on obtient d’abord la définition standard : "DADDCRDAA;DAADDRDAAA;DAAADDCCRDAAAA;DAAAADDRDA". Et à partir de là, le nombre de définition : 31332531173113353111731113322531111731111335317.

L’intérêt de cette notation de Turing, c'est qu'on va disposer maintenant d'un nombre qui décrit une machine de Turing, et on va pouvoir analyser ce nombre. En particulier, Turing va chercher à savoir si il est possible de prédire, à partir du nombre de la définition d'une machine prise au hasard, si cette machine va aboutir à un résultat ou pas. C’est le problème de l’arrêt. 

Si la machine calcule bien, elle finit par s’arrêter une fois qu’elle a terminé son calcul, mais il existe certaines machines qui tournent en boucle, et il est facile d’imaginer une machine qui inscrirait successivement les même caractères sur le même carré de la bande, sans jamais s’arrêter, et sans jamais s’approcher d’un quelconque résultat. C’est ce que Turing appelle les machines « circulaires », puisqu’il y a cette idée de « tourner en boucle ».

Alors comme pour le problème de la décision, on peut supposer qu’il est impossible de décider à l'avance si une machine de Turing que l'on choisit aléatoirement, appelons-la la machine M, si cette machine M est circulaire, donc qu'elle ne s'arrête jamais, ou bien si elle est sans cercle, c'est à dire qu'elle s'arrête.

Pour résoudre ce problème, nous allons imaginer, comme Turing, qu’il existe une machine, qu’on appellera la machine D, qui soit capable de décider si la machine M, celle que l’on veut tester, est circulaire ou non.

Alors comment fonctionne cette machine D ? Sur le ruban de la machine D, on inscrit le nombre de description de la machine M, et, la machine D, si elle décide que la machine M est circulaire, inscrira sur le ruban le symbole « u ». Sinon, elle écrira le symbole « s ».

Ensuite, Turing nous propose de construire la machine U. Son fonctionnement est simple, si elle lit sur son ruban le symbole "u", elle s'arrête immédiatement. mais si elle lit le symbole "s", la machine va alors rentrer dans une configuration de boucle infinie, et ne s’arrêtera donc pas.

En associant les machines D et U, on peut créer la machine H. Donc, pour récapituler le fonctionnement de la machine H, on lui fournit d'abord le nombre de description d'une machine M, puis la machine H va calculer un premier résultat intermédiaire, et inscrira sur son ruban soit le symbole « u », soit le symbole « s », qui dépends de la « circularité » de la machine M. Enfin, la machine H va soit s’arrêter immédiatement, soit entrera dans un état de boucle infinie, et fonction de ce résultat intermédiaire.

Et c’est à ce moment, après avoir patiemment détaillé la construction de cette machine H, que Turing nous pose la question suivante : que se passe-t-il si on donne à la machine H sa propre définition ?

Et bien il n’y a que deux issues possibles, soit, le résultat intermédiaire prédit que la machine H est circulaire, donc on écrit le symbole « u » sur la bande, puis la machine s’arrête. Elle n’est donc pas circulaire, et la prédiction du résultat intermédiaire, était donc fausse, puisque la prédiction était que la machine ne s’arrêterait pas, or, elle s’est arrêtée.

Et puis on a l’autre cas, celui où le résultat intermédiaire prédit que la machine H n’est pas circulaire, donc qu’elle s’arrêtera. Dans ce cas, c’est le symbole « s » qui est écrit sur la bande, et la machine H va alors entrer dans une configuration de boucle infinie, elle est circulaire.

Les prédictions du résultat intermédiaire, qui sont en fait les prédictions de la machine D qu’on avait imaginée, sont donc dans les deux cas en erreur. On est face à une situation qui ne peux pas se résoudre, ce qui fera à Turing que la machine D ne peux pas exister. Il est donc impossible de décider à l’avance si une machine de Turing donnée est circulaire ou pas.

Et c’est avec un raisonnement quasi-identique que Turing va également pouvoir affirmer qu’on ne peux pas décider si une machine de Turing va ou non écrire le caractère « 0 » au cours de son calcul.

Alors ces conclusions sont importantes, puisque Turing va montrer dans la suite de son article, dans une démonstration que je n’ai pas le temps d’expliquer ici parce qu’elle est un peu compliquée et demande une certaine maitrise de l’outil mathématique, que si il existe une machine, donc une procédure effective, capable de décider si une formule mathématique est prouvable, alors il existe une procédure effective pour décider si une machine de Turing inscrit le symbole « 0 » ou non. 

Et non savons déjà que ça n’est pas le cas, alors comme Turing, nous pouvons conclure que le problème de la décision d’Hilbert n’a pas de solution, il ne peux pas être résolu, on ne peux pas déterminer de procédure effective pour décider la démontrabilité d’une formule mathématique.

L’article de Turing continue encore, il va faire maintenant ce qui manquait aux propositions de Church, il va justifier, d’un point de vue épistémologique et philosophique, le modèle de calcul que représentent ses machines. Il va les comparer au cahier quadrillés de mathématiques qu’utilisent les écolier lorsqu’ils apprenant à calculer. Et il ira même jusqu’à dire que le fonctionnement de ses machine est une bonne description du fonctionnement de la pensée humaine.

Alors je le signale, puisque j’avais dit que l’article allait être extrêmement important dans le monde des sciences, fondateur même, que pour l’instant, il n’y a rien de très nouveau. Turing propose une définition de la calculabilité, mais Church l’avait fait avant lui, et il trouve la solution négative au problème de la décision, ce que Church avait également fait ! 

Certes, le modèle de Turing est plus simple et plus intuitif que le λ-calcul, et je pense que personne ne me contredira là dessus, pas même Church que je cite : « l'identification avec l'effectivité dans l'ordinaire [...] semble immédiatement évident ».

Gödel également va ajouter une note aux théorèmes d’incomplétudes : « Grâce à certains travaux qui ont suivi cet article, en particulier ceux de A. M. Turing, nous disposons désormais d'une définition sûre, précise et adéquate du concept de système formel [...] dont la propriété est qu'en son sein, et en principe, le raisonnement peut être entièrement remplacé par des règles mécaniques ».

Mais ce n’est pas seulement sa simplicité qui va donner à cet article sa renommée, c’est aussi grâce à un chapitre dont je ne vous ai pas encore parlé, où Turing nous explique le concept de ce qu’il appelle la machine « universelle ».

Je vous lis la première phrase de ce chapitre : "Il est possible d'inventer une machine unique qui peut être utilisée pour calculer toute séquence calculable. [...] Si cette machine U est fournie avec une bande sur laquelle est écrite la définition standard d'une machine M, U calculera la même séquence que M".

C’est absolument extraordinaire ce que nous dit Turing. Et il faut se replacer dans le contexte de l’époque pour en saisir toute la portée. Les machines que l’on construit à cette époque sont construites dans un but particulier, pour réaliser une opération précise, un petit peu à l’image des machines de Turing standard.

Mais il est possible de créer une machine unique, qui soit capable de réaliser n’importe quelle opération, n’importe quelle tâche, n’importe quel calcul, à condition d’inscrire la description de ce calcul sur la bande de la machine. C’est à dire de lui fournir un programme, que la machine va lire, puis exécuter.

Turing donne d’ailleurs la table d’instruction d’une telle machine dans le chapitre suivant.

Et c’est en ça que le concept de machine universelle est révolutionnaire, et que l’article de Turing est fondateur, c’est parce qu’il pose là, d’un seul coup, les deux concepts qui vont devenir les deux piliers théoriques de la science informatique qui n’existe pas encore, à savoir : une machine unique capable de réaliser n’importe quel calcul, et un programme stocké dans ce qu’on pourrait appeler la mémoire de la machine, en tout cas qui est accessible et modifiable par cette machine, et on pourrait même se prendre à imaginer une machine qui se re-programmerait elle-même.

De là à dire que Turing ait inventé l’ordinateur, il n’y a qu’un pas, que certains franchissent mais que je n’oserait pas. Entre la machine de Turing universelle et l’ordinateur, si ils sont identiques d’un point de vue théorique, ils sont tout à fait différents du point de vue de leur construction, et faire le raccourci un peu trop évident de dire que finalement la machine de Turing universelle et l’ordinateur ne sont qu’une seule et même chose, ça serait mettre de côté tout le travail de ceux qui vont inventer les techniques, et les technologies, qui vont permettre la construire les ordinateurs.

D’ailleurs, et c’est là, je crois, le meilleur exemple de ce que je viens de dire, Turing lui-même, après avoir terminé son article, avait voulu, il avait était immédiatement intéressé par la construction d’une machine qui soit l’incarnation du concept de machine universelle. Mais il ne le fera pas, et la raison est simple, il ne connait pas, en 1936, de technologie suffisamment capable pour mener à bien un tel projet.

Et c’est sur ce point que se concentrera le second chapitre de cet exposé, sur la partie pratique de notre histoire, le travail des ingénieurs, c’est l’invention et l’évolution de machines, concrètes cette fois, qui servent à calculer.
