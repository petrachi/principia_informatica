En 1902, le mathématicien Bertrand Russell réfléchit à la théorie des ensembles. Il se dit : L’ensemble des cuillères à thé n’est lui-même une cuillère à thé, il ne s’appartient donc pas à lui-même. Mais si je prends l’ensemble des ensemble qui ne s’appartiennent pas à eux-mêmes, et bien si cet ensemble s’appartenait à lui-même, il devrait respecter sa définition de ne pas s’appartenir à lui-même, mais s’il ne s’appartient pas à lui-même, alors il mériterais de faire partie de l’ensemble de tous les ensembles qui ne s’appartiennent pas à eux mêmes, mais alors, il s’appartiendrait à lui-même.

Dans l’université de Göttingen, David Hilbert, le grand mathématicien Allemand, qui à déjà résolu le problème des invariants, qui a publié un rapport sur la théorie des nombres algébriques et qui à également repensé les règles de la géométrie, n’est pas surpris. Il semblerait qu’à Göttingen, un autre mathématicien ait déjà eu la même idée que Russell, et que le paradoxe y soit déjà connu depuis déjà trois ou quatre ans.

Il n’empêche que ce paradoxe de Russell pose problème, d’autant plus que ce n’est pas le seul, en 1897, le mathématicien italien avait déjà identifié un paradoxe avec les nombres ordinaux, et en 1905, Antoine Richard lui aussi va exposer un paradoxe en théorie des ensembles grâce à la méthode de diagonalisation de Cantor, Cantor qui avait inventé justement cette théorie des ensembles, c’est un comble.

Le problème des paradoxes, c’est qu’ils révèlent que l’axiomatique, c’est à dire, l’ensemble des règles mathématiques, que l’axiomatique donc de la théorie des ensembles n’est pas consistante, c’est à dire, que les différents axiomes, les différentes règles de la théorie, sont en vérité contradictoires.

Mais le problème est même plus grand que cela, puisque la consistance de la théorie des ensemble reposait en fait, comme il était de coutume à l’époque, sur la consistance des axiomes de l’arithmétique. Alors, si on a pu montrer, grâce aux paradoxes, que la théorie des ensemble n’est pas consistante, cela signifie aussi que l’arithmétique, la base de tout l’édifice mathématique, ne l’est pas non plus. Alors il va s’agir, à partir de ce moment là, pour les mathématiciens de l’époque, de « réparer » les mathématiques, de repenser les règles de l’arithmétique, et ce sera le début de la quête du fondement.

Les points de vue sont partagés. Pour les intuitionnistes, les paradoxes étaient un avertissement : un usage abusif des règles de la logique, qui font abstraction du sens des notions, crée des énoncés vides de sens, et finalement, des paradoxes. Il faudrait alors retirer des mathématiques tout ce qui n’est pas immédiatement évident. 

En particulier, c’est la notion de l’infini qui va poser problème, puisque l’infini présuppose l’existence en soi des nombres, et des différents objets mathématiques. L’infini, finalement, c’est l’ensemble de tous les nombres. Et d’ailleurs, la notion d’infini est présente dans chacun des trois paradoxes, alors pour les intuitionnistes, ça ne fait pas de doute, il faut supprimer l’infini, et même si pour ça, ils doivent se séparer de certains outils parmi les plus utiles des mathématiques, comme par exemple le principe du tiers exclu.

David Hilbert, qui ne partage pas ces idées, sera très critique par rapport aux intentions des intuitionnistes, il va dire : « Oter le [tiers exclu] au mathématicien serait comme si on voulait enlever à l'astronome son télescope, au boxeur le droit de se servir de son poing. [... C'est] quasiment à renoncer à la science mathématique. Car que sont les misérables restes, qu'est-ce que la poignée de résultats incomplets et disparates que les intuitionnistes ont pu élaborer [...] à l'égard de l'étendue formidable de la mathématique contemporaine ? »

Il faut dire que Hilbert, contrairement aux intuitionnistes, croit en la résolubilité de tout problème mathématique, il avait dit au congrès international des mathématiciens en 1900 : « Jamais le mathématicien ne sera réduit à dire Ignorabimus ! ». Ignorabimus, la citation latine qui signifie « on ne sait pas, et on ne saura pas ». Donc jamais le mathématicien ne sera réduit à dire « on ne sait pas et on ne saura pas », autrement dit, il n’y a pas de problème mathématique impossible à résoudre.

Et justement, cette idée se défend dans le principe du tiers exclu, puisque le tiers exclu permet au mathématicien d’affirmer que soit une proposition mathématique est vraie, soit son inverse est vraie, il n’y a pas de troisième possibilité. Alors, comme le disait Brouwer, un des patrons de l’intuitionnisme : « La question de la validité du tiers exclu équivaut à celle de la possibilité de problèmes mathématiques non résolubles ».

Un autre qui partage assez les idées des intuitionnistes, c’est Henri Poincaré. Lui aussi ne croit pas en l’existence en soi des objets mathématiques, il pense plutôt que les nombres sont construits par le mathématicien, et il dénonce dans le paradoxes le principe du cercle vicieux. Selon lui, les paradoxes reposent tous sur un cercle vicieux, des objets qui s’auto-référencent, ils ont besoin de déjà exister pour pouvoir exister.  Alors il faudrait pouvoir prévenir dans l’usage des mathématiques l’apparition de ce genre de cercle vicieux.

C’est justement l’idée qu’on eu Russell et Whitehead, qui vont tenter une axiomatisation de l’ensemble des mathématiques basée sur la théorie des types. Cette théorie, développée par Russell quelques années auparavant cherche à hiérarchiser les différentes notions mathématiques. Ainsi, les notions de rang 1 ne peuvent s’appliquer qu’aux notions de rang 2, celle de rang 2 s’appliquent à celles de rang 3, etc. Le but bien entendu est d’éviter l’apparition d’un cercle vicieux comme décrit par Poincaré.

Ce sera finalement, après dix ans de travail que les deux hommes vont publier un colossal ouvrage en trois volumes, ce sont les « Principia Mathematica », dans lesquelles figurent pas moins de quatre cents cinquante pages afin de prouver que « 1+1 = 2 ».

Cependant, les «  Principia Mathematica » vont avoir du mal à convaincre. Le principal reproche qu’on leur fait est le manque d’une preuve de consistance, une preuve formelle que les axiomes de Russell et Whitehead ne sont pas contradictoires. Russell va se défendre en disant que les axiomes sont des notions intuitives, évidentes, et nécessairement vraies. Une preuve de consistance ne serait qu’une simple validation de l’évidence, et n’est pas nécessaire.

David Hilbert, qui s’était déjà opposé aux intuitionnistes, va commenter à propos des « Principia Mathematica » : « La théorie des fondements de Russell et Whitehead [...] fait reposer les mathématiques sur l'axiome de l'infini et sur un axiome dit de réductibilité. Or ces deux axiomes sont d'authentiques hypothèses qui ne peuvent s'appuyer [...] sur une preuve de consistance [... et] dont la validité universelle reste même ouverte au doute ».

Pourtant Hilbert, qui lui s’est engagé dans le programme formaliste, n’a pas non plus trouvé la solution au problème du fondement, même si il a des raisons d’être optimiste. En 1928, au congrès international de Bologne, il va présenter les objectifs du programme formaliste. L’idée de Hilbert, qui entre temps à permis l’émergence de la mécanique quantique, à d’abord été de définir de nouveaux axiomes afin de définir l’arithmétique, et il cherche maintenant à prouver trois choses : l’axiomatique formelle, c’est l’axiomatique de Hilbert, l’axiomatique formelle est-elle consistante ? Est-elle complète ? Et est-elle décidable ?

Alors il est bien entendu que Hilbert espère apporter une réponse positive à ces trois questions. Et c'est même à cette condition seulement que pourrait se concrétiser le programme formaliste, ce qui semble plutôt bien parti. Toujours à la conférence de Bologne, Hilbert annonce d’abord qu’Ackerman et Von Neumann, deux mathématicien également engagés dans le programme formaliste, sont déjà parvenus à prouver la consistance de l’axiomatique formelle. Ensuite, qu’Ackerman et lui-même seront bientôt en mesure d’apporter la preuve de la complétude des « prédicats de premier ordre », qui, si ils ne représentent pas toutes l’arithmétique, représentent tout de la même une des parties les plus importantes puisqu’ils représentent les règles de la logique mathématique.

Pourtant, c’est finalement Kurt Gödel, un jeune logicien autrichien, qui va finir par apporter cette preuve de la complétude des « prédicats de premier ordre ». Et c’est lui aussi qui va, un an plus tard, présenter, au cours d’une discussion informelle qui se tient sur les côtés d’une conférence à Köningsberg, un théorème nouveau à quelques une de ses pairs, parmi lesquels figure John Von Neumann. Voici le théorème : « Si l'arithmétique élémentaire est ω-consistante, elle comporte des formules fermées qui ne sont ni démontrables, ni réfutables à partir des axiomes ». Von Neumann, qui rappelons-le est largement impliqué dans le programme formaliste d’Hilbert s’exclamera : « Tout est fini ».

C’est le seul d’ailleurs, à ce moment, à réaliser les implications du théorème de Gödel. Il va même lui écrire quelques mois plus tard qu’il à trouvé un second théorème à partir du premier : « la consistance d'un système ne peut pas être établie au moyen de raisonnements qui se laisseraient formaliser dans le système ».

Gödel était en réalité déjà arrivé à la même conclusion, il avait envoyé quelques jours plus tôt un manuscrit avec les deux théorèmes à une revue mathématique Viennoise qui les publiera finalement en 1931.

Ces deux théorèmes, qui sont connus depuis comme les théorèmes d’incomplétude de Gödel, sont quasi catastrophiques pour le programme formaliste, et il semblerait même que Hilbert soit entré dans une colère noire à la lecture des théorèmes de Gödel, sans doute parce qu’il n’avait pas trouvé lui-même ce que Gödel considère comme une « conséquence presque triviale » des travaux de Skolem.

Exposés simplement, les théorèmes de Gödel disent ceci : une axiomatique, quelque qu’elle soit, si elle est suffisamment expressive pour représenter l’arithmétique élémentaire, ne peut pas être à la fois complète et consistante. Et, qu’il est impossible d’apporter une preuve de consistance de l’arithmétique à l’aide d’un système mathématique minimaliste. Or souvenez-vous que le but du programme formaliste était de démontrer la complétude et la consistance de l’axiomatique formelle, ceci à l’aide de la mathématique finitiste. Ces deux objectifs du programme formalistes sont en fait irréalisables.

Pourtant, Hilbert avait déjà annoncé en 1928 qu’Ackermann et Von Neumann avaient réussi à prouver la consistance de l’axiomatique formelle, Von Neumann réalisera en fait qu’il y avait une erreur de calcul dans cette preuve.

Alors, en 1931, après toutes les différentes tentatives pour répondre au problème du fondement, il semble ne rester plus grand chose à faire, même si Gödel dira : « il reste un espoir pour que dans le futur on puisse trouver des méthodes satisfaisantes [...] dépassant les limites du système [finitiste de Hilbert] et permettant de fonder l'arithmétique classique et l'analyse. Cette question ouvre un champ de recherches fécond ». Il ajoutera même, comme pour rassurer Hilbert : « la conviction dans la décidabilité de tout problème mathématique n'est pas ébranlée par ce résultat ».

Von Neumann commentera lui aussi : « Ce résultat imposant de l'analyse de Gödel ne doit pas être mal compris: il n'exclut pas une preuve méta-mathématique de la consistance de l'arithmétique. [...] Gödel a montré qu'aucune preuve n'est possible qui peut être représentée au sein de l'arithmétique. Son argument ne supprime pas la possibilité de preuves strictement finitiste qui ne sont pas représentés au sein de l'arithmétique. Mais personne aujourd'hui ne semble avoir une idée claire de ce qu'une preuve finitiste serait qui ne serait pas formulable dans l'arithmétique ». 

Et d’ailleurs, le programme formaliste va continuer à faire l’objet d’études, puisque souvenez, il s’agissait, en plus de démontrer la complétude et la consistance de l’axiomatique formelle, d’en démontrer également la décidabilité. C’est à dire, est-ce qu’il existe une « procédure effective » qui permette de décider si une formule mathématique quelconque est démontrable ou pas. C’est cette idée de « procédure éffective » qui va remettre la question de la décidabilité au centre des mathématiques, puisque cette idée, qui  ne possède pas encore de définition mathématique précise, et qui renvoie en fait à la notion de « calculabilité », doit être définie pour répondre à la fois au problème de la décision du programme formaliste, mais aussi pour permettre la généralisation des théorème d’incomplétude de Gödel.

Au cours du 18ème siècle, les mathématiciens avaient associé l’idée de « calculabilité » à la notion mathématique de fonction. Mais les fonction ont évoluées au cours du 19ème siècle pour ne plus décrire qu’une transition entre un ensemble de départ et un ensemble d’arrivé, sans qu’un calcul ne soit nécessairement attaché à la fonction. Alors il va s’agir maintenant pour les mathématiciens de tracer la limite entre les fonctions qui sont calculables, et celles qui ne le sont pas.

C’est finalement en 1936 que les deux premières définitions de la « calculabilité » font leur apparition. D’abord, c’est Alonzo Church, installé à l’université de Princeton aux États-unis, qui va définir ce qu’il appelle la « calculabilité effective » à l’aide d’un système de calcul qu’il à développé quelques années auparavant, le λ-calcul. Mais le λ-calcul est complexe, et la relecture et la vérification des travaux de Church n’est pas aisée pour le reste de la communautés mathématique.

De l’autre côté de l’Atlantique, en Angleterre, Alan Turing, qui à alors 24 ans va proposer lui aussi, tout à fait indépendamment, une définition de la calculabilité, plus simple et plus évidente, qui fera rapidement l’unanimité dans la communauté, Church même reconnaitra l’évidence immédiate du modèle de ce qu’il appellera les machines de Turing, puisque Turing lui-même ne désigne ses machine qu’en terme de machines automatiques, même si les deux définitions sont tout à fait équivalentes.

Alors on peut-être tout de même surpris de voir mentionner, dans l’article de Turing, les travaux de Church. Mais il faut signaler que si Turing aura finit la rédaction de son article à la fin de l’été 1936, l’article ne sera finalement publié qu’en janvier 1937, et entre temps, Turing sera allé rejoindre Church à Princeton, sous l’influence de son ancien professeur à Cambridge, Max Newman, qui avait écrit à Church : « Je dois mentionner que le travail de Turing est entièrement indépendant: il a travaillé sans supervision ou critique de quiconque. Cela rend d'autant plus important qu'il devrait entrer en contact le plus tôt possible avec les principaux travailleurs de ce domaine, de sorte qu'il ne devrait pas se transformer en un solitaire confirmé ».

C’est d’ailleurs Max Newman qui avait aussi mis Turing sur la piste d’une machine pour décrire la calculabilité. William Newman, le fils du professeur, se rappelle : « À un moment, il a posé à sa classe une question: la prouvabilité des énoncés mathématiques pourrait-elle être découvert par un procédé mécanique ? Cette question est restée dans l'esprit d'Alan, et peu à peu, il a développé le papier qui allait faire sa réputation dans le monde mathématique, et plus tard en science informatique : Sur les nombres calculables, avec une application au problème de la décision ».

Turing définit dans son article la calculabilité à l’aide d’un certain type de machines, tout ce qui peut être calculé par ces machines définit l’ensemble de ce qui est calculable, le reste, n’est pas calculable. La description de ces machines est faite dés le premier chapitre de l’article : « On peut comparer un homme en train de calculer un nombre à une machine qui est seulement capable d'un nombre fini de conditions, qui seront appelés "m-configurations". La machine est livrée avec une "bande" (l'analogue du papier) qui la traverse, et est divisée en sections (appelé "carrés") capables chacun de porter un "symbole". A tout moment, il y a juste un carré [...] qui est "dans la machine". Nous pouvons appeler cette place le "carré lu". Le symbole sur le carré lu peut être appelé le "symbole lu". Le "symbole lu" est le seul dont la machine est, pour ainsi dire, "directement au courant". Cependant, en modifiant sa m-configuration la machine peut se rappeler efficacement certains des symboles qu'elle a "vu" précédemment. À tout moment, le comportement de la machine est déterminé par la m-configuration [...] et [...] le symbole lu. Cette paire [...] sera appelé "configuration" : ainsi la configuration détermine le comportement de la machine. Dans certaines configurations dans lesquelles le carré lu est vierge (i.e. il ne porte pas de symbole), la machine écrit un nouveau symbole sur le carré lu: dans d'autres configurations, il efface le symbole lu. La machine peut également changer le carré en cours de lecture, mais seulement en le déplaçant d'une case à droite ou à gauche. En plus de l'une de ces opérations, la m-configuration peut être modifiée. Certains des symboles écrits formeront la séquence de chiffres [...] du nombre [...] qui est calculé. Les autres ne sont que des brouillons pour "aider la mémoire". [...] Je soutiens que ces opérations comprennent toutes celles qui sont utilisés dans le calcul d'un nombre. »

Ensuite, Turing donne un premier exemple de calcul à l’aide de ses machines : « Une machine peut être construite pour calculer la séquence 01010101 ... [...] La machine possède quatre m-configurations "b", "c", "f", "e" et est capable d'écrire "0" et "1". Le comportement de la machine est décrite dans le tableau suivant dans lequel "R" signifie "la machine se déplace de sorte à lire le carré immédiatement à la droite de celui qui était auparavant lu". De même, pour "L". "E" signifie "le symbole lu est effacé" et "P" signifie "écrire". La machine démarre dans la m-configuration "b" et avec une bande vide. [b None P0, R c; c None R e; e None P1, R f; f None R b] »

La suite de l’article servira à répondre au problème de la décision, la réponse est bien entendu négative. Turing va montrer qu’il est impossible de décider à l’avance si un énoncé mathématique est démontrable. Pour ça, il utilise un problème similaire, le problème de l’arrêt. Il cherche à savoir si l’on peut prédire l’arrêt d’une machine de Turing choisie aléatoirement, à partir de sa description seulement. Pour montrer que c’est impossible, il va d’abord imaginer que c’est possible. Si on possédait une machine, que l’on appellera la machine D, qui soit capable de prédire si une machine prise au hasard, la machine M, s’arrête, on pourrait lui greffer un appendice, une machine qui, si la machine D prédit « oui », ne s’arrêtera pas, et si la machine D sit « non », s’arrêtera. À partir de là, que se passe t-il si on fournit cette machine sa propre description. Si la prédiction est qu’elle s’arrêtera, elle ne s’arrête pas, et si la prédiction est qu’elle ne s’arrêtera pas, elle s’arrête. Turing conclut donc que la machine D ne saurait exister, qu’il est donc impossible de prédire l’arrêt d’une machine de Turing, et, puisque les deux problèmes sont équivalents, qu’il est également impossible de décider si une formule mathématique est démontrable.

Mais ce qui, pour moi, est le plus intéressant dans l’article de Turing, ce n’est ni sa définition de la calculabilité, ni la solution au problème de la décision, c’est la description au sixième et septième chapitre de l’article du concept de « machine universelle ». Turing écrit : « Il est possible d'inventer une machine unique qui peut être utilisée pour calculer toute séquence calculable. [...] Si cette machine U est fournie avec une bande sur laquelle est écrite la définition standard d'une machine M, U calculera la même séquence que M ».

Alan Turing vient en réalité de poser les bases théoriques de la science informatique à venir. On est en 1936 à une époque où les machines à calculer, mécaniques celles-là, sont construites pour ne réaliser qu’une opération spécifique, Turing dit qu’il est théoriquement possible d’en construire une, unique, qui soit capable de réaliser tous les calculs, à condition de fournir à la machine la description du calcul à effectuer, c’est à dire, de lui fournir un programme. Et puisque, dans le modèle de Turing, le programme est inscrit sur la bande de la machine, cela signifie que la machine peut elle-même modifier son propre programme, et il n’est pas exclu d’imaginer une machine qui se re-programmerait elle-même.

De là à dire que Turing ait inventé l’ordinateur, il n’y a qu’un pas, que certains franchissent mais que je n’oserait pas. Entre la machine de Turing universelle et l’ordinateur, si ils sont identiques d’un point de vue théorique, ils sont tout à fait différents du point de vue de leur construction, et faire le raccourci un peu trop évident de dire que finalement la machine de Turing universelle et l’ordinateur ne sont qu’une seule et même chose, ça serait mettre de côté tout le travail de ceux qui vont inventer les techniques, et les technologies, qui vont permettre la construire les ordinateurs.

D’ailleurs, et c’est là, je crois, la meilleure preuve, Turing lui-même, après avoir terminé son article, avait voulu, il avait était immédiatement intéressé par la construction d’une machine qui soit l’incarnation du concept de machine universelle. Mais il n’en fera rien, puisqu’il n’existe pas, à sa connaissance, de technologie qui lui permettrait de mener son projet à bien.

Alors il va finalement entrer en contact avec la technologie électronique pendant la seconde guerre mondiale, alors qu’il travaille pour les services secrets britanniques au centre de Bletchley park. Après la guerre, il se concentrera à la réalisation d’un des premiers ordinateurs, mais cela, c’est une histoire pour une autre fois.